#!/usr/bin/env python
# -*- coding:utf-8 -*-
"""
FinBoom ç»Ÿä¸€è¯„ä¼°è„šæœ¬

æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
1. æœ¬åœ°éƒ¨ç½²æ¨¡å‹ï¼ˆä½¿ç”¨ vLLMï¼‰
2. APIæ¨¡å‹ï¼ˆOpenAI, Anthropic, Google, Qwenç­‰ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
  # æœ¬åœ°æ¨¡å‹
  python eval/eval.py --model-type local --model-path /path/to/model ...

  # APIæ¨¡å‹
  python eval/eval.py --model-type api --provider openai --model-name gpt-4 ...
"""

import json
import os
import re
import sys
import math
import time
import argparse
import traceback
from typing import List, Dict, Any, Optional, Tuple, Union
import numpy as np

# ==========================================================
# 0. å¯¼å…¥ä¾èµ–
# ==========================================================
current_script_dir = os.path.dirname(os.path.abspath(__file__))
finboom_root = os.path.join(current_script_dir, '..')
sys.path.insert(0, finboom_root) 

try:
    from data.tool_library import *
except ImportError:
    print("æ‰¾ä¸åˆ° tool_library, è¯·ç¡®ä¿æ‚¨åœ¨ 'finboom' æ ¹ç›®å½•ä¸‹è¿è¡Œ")
    sys.exit(1)

# å¯é€‰å¯¼å…¥ vLLMï¼ˆä»…æœ¬åœ°æ¨¡å‹éœ€è¦ï¼‰
try:
    from vllm import LLM, SamplingParams
    VLLM_AVAILABLE = True
except ImportError:
    VLLM_AVAILABLE = False

# å¯é€‰å¯¼å…¥ API åº“ï¼ˆä»…APIæ¨¡å‹éœ€è¦ï¼‰
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    from anthropic import Anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

try:
    import google.generativeai as genai
    GOOGLE_AVAILABLE = True
except ImportError:
    GOOGLE_AVAILABLE = False


# ==========================================================
# 1. APIæ¨¡å‹åŒ…è£…å™¨
# ==========================================================
class APIModelWrapper:
    """APIæ¨¡å‹åŒ…è£…å™¨ï¼Œç»Ÿä¸€æ¥å£"""
    
    def __init__(self, provider: str, model_name: str, api_key: str = None, base_url: str = None):
        self.provider = provider.lower()
        self.model_name = model_name
        self.api_key = api_key or os.getenv(f"{provider.upper()}_API_KEY")
        self.base_url = base_url
        
        if self.provider == "openai":
            if not OPENAI_AVAILABLE:
                raise ImportError("éœ€è¦å®‰è£… openai: pip install openai")
            self.client = openai.OpenAI(api_key=self.api_key, base_url=self.base_url)
        elif self.provider == "anthropic":
            if not ANTHROPIC_AVAILABLE:
                raise ImportError("éœ€è¦å®‰è£… anthropic: pip install anthropic")
            self.client = Anthropic(api_key=self.api_key)
        elif self.provider == "google":
            if not GOOGLE_AVAILABLE:
                raise ImportError("éœ€è¦å®‰è£… google-generativeai: pip install google-generativeai")
            genai.configure(api_key=self.api_key)
            self.client = genai.GenerativeModel(model_name)
        elif self.provider == "qwen":
            if not OPENAI_AVAILABLE:
                raise ImportError("éœ€è¦å®‰è£… openai: pip install openai")
            self.client = openai.OpenAI(
                api_key=self.api_key,
                base_url=base_url or "https://dashscope.aliyuncs.com/compatible-mode/v1"
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„provider: {provider}")
    
    def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """ç»Ÿä¸€çš„èŠå¤©æ¥å£"""
        if self.provider == "openai" or self.provider == "qwen":
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                **kwargs
        )
            return response.choices[0].message.content
        elif self.provider == "anthropic":
            system_message = None
            conversation = []
            for msg in messages:
                if msg["role"] == "system":
                    system_message = msg["content"]
                else:
                    conversation.append(msg)
            
            response = self.client.messages.create(
                model=self.model_name,
                max_tokens=kwargs.get("max_tokens", 2048),
                system=system_message,
                messages=conversation
            )
            return response.content[0].text
        elif self.provider == "google":
            prompt = ""
            for msg in messages:
                role = msg["role"]
                content = msg["content"]
                if role == "system":
                    prompt += f"System: {content}\n\n"
                elif role == "user":
                    prompt += f"User: {content}\n\n"
                elif role == "assistant":
                    prompt += f"Assistant: {content}\n\n"
            
            response = self.client.generate_content(prompt)
            return response.text
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„provider: {self.provider}")


# ==========================================================
# 2. å·¥å…·å‡½æ•°
# ==========================================================
def format_tool_tree_for_prompt(tool_tree_dict, indent_level=0):
    """æ ¼å¼åŒ–å·¥å…·æ ‘ä¸ºMarkdown"""
    markdown_str = ""
    indent = "  " * indent_level
    for key, value in tool_tree_dict.items():
        if key == "_description":
            continue
        if isinstance(value, dict):
            description = value.get("_description", "")
            markdown_str += f"{indent}- **{key}**: {description}\n"
            if isinstance(value.get("children"), dict):
                markdown_str += format_tool_tree_for_prompt(value["children"], indent_level + 1)
        else:
            markdown_str += f"{indent}- **{key}**\n"
    return markdown_str


def build_final_answer_prompt(tool_tree_dict: dict, all_tools_description: list) -> str:
    """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
    tool_hierarchy_text = format_tool_tree_for_prompt(tool_tree_dict)
    tools_json_str = json.dumps(all_tools_description, indent=2, ensure_ascii=False)
    template = """You are a professional financial question answering assistant. Your task is to use tools to find the answer and provide ONLY the final result in a specific format.

# Workflow
1.  **Thinking & Tool Use**: Use the provided tools in a step-by-step manner to gather all necessary information. You can think and call tools for multiple turns.
2.  **Final Answer**: Once you have all the information, your **FINAL response MUST ONLY contain the `\\boxed{{}}` expression** and nothing else.

# Final Answer Requirements
- Your last response must start with `\\boxed{{` and end with `}}`.
- Inside `\\boxed{{}}`, place the final answer value(s) directly.
- **DO NOT** wrap the answer in a list `[]`.
- If the question asks for multiple values, separate them with a comma `,`.

---
### Interaction Example

#### User Question
What are the latest opening prices for SF Express's A-share (sz002352) and HK-share (0270)?

#### Your Response (First Turn)
Thinking:
I need to find the latest opening price for two different stocks in two different markets. I will use `get_a_stock_daily_price` for the A-share and `get_hk_stock_daily_price` for the HK-share.

Action:
<tool>get_a_stock_daily_price({{"code": "sz002352", "column_label": "open", "query_date": "latest"}})</tool>
<tool>get_hk_stock_daily_price({{"code": "0270", "column_label": "open", "query_date": "latest"}})</tool>

#### (System returns tool results to you: A-share price is 40.2, HK-share price is 7.2)

#### Your Response (Second and FINAL Turn) Put the answer in \\boxed{{}} (value, string, date) in the end of the answer, for example:
\\boxed{{40.2, 7.2}}
---

## Tool Hierarchy
{hierarchy}

## Available Tools (JSON Schema)
{tools_json}
"""
    system_prompt = template.format(hierarchy=tool_hierarchy_text, tools_json=tools_json_str)
    return system_prompt


def _numpy_converter(obj):
    """JSONåºåˆ—åŒ–numpyç±»å‹"""
    if isinstance(obj, np.generic):
        return obj.item()
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    raise TypeError(f'Object of type {obj.__class__.__name__} is not JSON serializable')


def execute_simplified_code(code_string: str, function_mapper_dict: dict) -> Tuple[bool, Any, Optional[str]]:
    """æ‰§è¡Œä»£ç å­—ç¬¦ä¸²"""
    indented_code = "\n".join(["    " + line for line in code_string.strip().splitlines()])
    temp_func_code = f"def temp_solve():\n{indented_code}"
    try:
        execution_context = globals().copy()
        execution_context.update(function_mapper_dict)
        exec(temp_func_code, execution_context)
        solve_function = execution_context.get('temp_solve')
        if not callable(solve_function):
            return False, None, "Failed to create a callable temporary function from the code string."
        result = solve_function()
        return True, result, None
    except Exception as e:
        return False, None, f"Execution failed: {traceback.format_exc()}"


def extract_boxed_answer(model_content: str) -> Tuple[Any, Optional[str]]:
    """ä»å“åº”ä¸­æå–\\boxed{}ç­”æ¡ˆ"""
    boxed_match = re.search(r"\\boxed{([^}]+)}", model_content)
    if not boxed_match:
        return None, "Error: No \\boxed{} found in the final response."
    boxed_answer_str = boxed_match.group(1).strip()
    eval_str = f"({boxed_answer_str},)" if ',' not in boxed_answer_str else f"({boxed_answer_str})"
    try:
        import ast
        result = ast.literal_eval(eval_str)
        return result[0] if len(result) == 1 else result, None
    except Exception as e:
        return None, f"Error: Failed to parse content inside \\boxed{{}}: {e}"


def compare_answers(model_answer: Any, ground_truth: Any, rel_tol=1e-5) -> bool:
    """æ¯”è¾ƒæ¨¡å‹ç­”æ¡ˆå’ŒåŸºå‡†ç­”æ¡ˆ"""
    model_tuple = model_answer if isinstance(model_answer, tuple) else (model_answer,)
    truth_tuple = ground_truth if isinstance(ground_truth, tuple) else (ground_truth,)
    if len(model_tuple) != len(truth_tuple):
        return False
    for m_item, t_item in zip(model_tuple, truth_tuple):
        if isinstance(m_item, (int, float)) and isinstance(t_item, (int, float)):
            if not math.isclose(m_item, t_item, rel_tol=rel_tol, abs_tol=1e-9):
                return False
        elif isinstance(m_item, float) and isinstance(t_item, float) and math.isnan(m_item) and math.isnan(t_item):
            continue
        elif m_item != t_item:
            return False
    return True


def extract_tool_calls(response: str) -> List[Dict[str, Any]]:
    """ä»å“åº”ä¸­æå–å·¥å…·è°ƒç”¨"""
    tool_calls = []
    pattern = r"<tool>(.*?)</tool>"
    matches = re.findall(pattern, response, re.DOTALL)
    
    for match in matches:
        try:
            func_name = match.split('(', 1)[0].strip()
            args_str = match.rsplit(')', 1)[0].split('(', 1)[1]
            arguments = json.loads(args_str)
            tool_calls.append({
                "name": func_name,
                "args": arguments
            })
        except Exception as e:
            print(f"    å·¥å…·è°ƒç”¨è§£æå¤±è´¥: {e}")
    
    return tool_calls


# ==========================================================
# 3. æœ¬åœ°æ¨¡å‹ Agent å¾ªç¯
# ==========================================================
def _truncate_messages(messages: List[Dict[str, str]], tokenizer: Any, max_len: int) -> List[Dict[str, str]]:
    """æˆªæ–­è¿‡é•¿çš„å¯¹è¯å†å²"""
    PROMPT_OVERHEAD_ESTIMATE = 50
    try:
        current_tokens = len(tokenizer.apply_chat_template(messages, add_generation_prompt=False))
    except Exception:
        current_tokens = sum(len(tokenizer.encode(m.get("content", ""))) for m in messages)
    if current_tokens + PROMPT_OVERHEAD_ESTIMATE <= max_len:
        return messages
    print(f"    [Agent è­¦å‘Š] å¯¹è¯å†å²è¿‡é•¿ (ä¼°ç®— {current_tokens} tokens)ï¼Œæ­£åœ¨è¿›è¡Œæˆªæ–­...")
    system_prompt = messages[0]
    CONVERSATION_TO_KEEP = 4 
    truncated_history = messages[-(CONVERSATION_TO_KEEP * 2):]
    new_messages = [system_prompt] + truncated_history
    try:
        new_tokens = len(tokenizer.apply_chat_template(new_messages, add_generation_prompt=False))
        print(f"    [Agent ä¿¡æ¯] æˆªæ–­åï¼Œå¯¹è¯å†å²é•¿åº¦ä» {current_tokens} å‡å°‘åˆ° {new_tokens} tokensã€‚")
    except Exception:
        pass
    return new_messages


def run_agent_loop_local(
    user_question: str,
    model: LLM,
    tokenizer: Any,
    sampling_params: SamplingParams,
    system_prompt: str,
    function_mapper: dict,
    max_turns: int
) -> Tuple[str, str, List[Dict], int]:
    """è¿è¡Œæœ¬åœ°æ¨¡å‹çš„Agentå¾ªç¯"""
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_question}]
    final_response = ""
    full_trajectory = f"--- USER ---\n{user_question}\n\n"
    tool_call_history = []
    error_count = 0
    
    try:
        model_max_len = model.llm_engine.model_config.max_model_len
    except AttributeError:
        model_max_len = 32768
        print(f"    [Agent è­¦å‘Š] æ— æ³•è‡ªåŠ¨è·å–æ¨¡å‹æœ€å¤§é•¿åº¦ï¼Œä½¿ç”¨é»˜è®¤å€¼: {model_max_len}")
    
    for turn in range(max_turns):
        print(f"    [Agent] æ­£åœ¨è¿›è¡Œç¬¬ {turn + 1}/{max_turns} æ¨ç†...")
        messages = _truncate_messages(messages, tokenizer, model_max_len)
        try:
            prompt_str = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        except Exception as e:
            error_msg = f"    [Agent é”™è¯¯] apply_chat_template å¤±è´¥: {e}"
            print(error_msg)
            return "", full_trajectory + error_msg, tool_call_history, error_count
        if len(tokenizer.encode(prompt_str)) >= model_max_len:
            error_msg = f"    [Agent é”™è¯¯] å³ä½¿æˆªæ–­åï¼Œæœ€ç»ˆçš„ prompt é•¿åº¦ä»ç„¶è¶…å‡ºæ¨¡å‹é™åˆ¶ã€‚"
            print(error_msg)
            return "", full_trajectory + error_msg, tool_call_history, error_count
        
        outputs = model.generate([prompt_str], sampling_params, use_tqdm=False)
        response_text = outputs[0].outputs[0].text.strip()
        messages.append({"role": "assistant", "content": response_text})
        final_response = response_text
        full_trajectory += f"--- ASSISTANT (æ€è€ƒä¸å·¥å…·è°ƒç”¨) ---\n{response_text}\n\n"
        print(f"    [Agent æ¨¡å‹å›å¤] {response_text[:150]}...")
        
        tool_call_matches = re.findall(r"<tool>(.*?)</tool>", response_text, re.DOTALL)
        if not tool_call_matches:
            print("    [Agent] æµç¨‹ç»“æŸ (æ¨¡å‹æœªè°ƒç”¨å·¥å…·ï¼Œå‡†å¤‡æä¾›æœ€ç»ˆç­”æ¡ˆ)ã€‚")
            return final_response, full_trajectory, tool_call_history, error_count
        
        tool_output_content = ""
        for match_str in tool_call_matches:
            tool_call_start = time.time()
            func_name = "unknown"
            arguments = {}
            try:
                func_name = match_str.split('(', 1)[0].strip()
                args_str = match_str.rsplit(')', 1)[0].split('(', 1)[1]
                arguments = json.loads(args_str)
                function_to_call = function_mapper.get(func_name)
                
                if function_to_call:
                    function_output = function_to_call(**arguments)
                    tool_call_end = time.time()
                    output_str = json.dumps(function_output, ensure_ascii=False) if isinstance(function_output, (dict, list)) else str(function_output)
                    tool_output_content += output_str
                    
                    tool_call_history.append({
                        "turn": turn,
                        "tool_name": func_name,
                        "tool_args": arguments,
                        "result": function_output,
                        "success": True,
                        "latency": tool_call_end - tool_call_start
                    })
                else:
                    error_count += 1
                    tool_call_end = time.time()
                    tool_output_content += f"Error: Tool '{func_name}' not found."
                    tool_call_history.append({
                        "turn": turn,
                        "tool_name": func_name,
                        "tool_args": arguments,
                        "result": {"error": f"å·¥å…· '{func_name}' ä¸å­˜åœ¨"},
                        "success": False,
                        "latency": tool_call_end - tool_call_start
                    })
            except json.JSONDecodeError as e:
                error_count += 1
                tool_call_end = time.time()
                error_msg = f"å·¥å…·å‚æ•°è§£æå¤±è´¥: {str(e)}"
                tool_output_content += error_msg
                tool_call_history.append({
                    "turn": turn,
                    "tool_name": func_name,
                    "tool_args": arguments,
                    "result": {"error": f"å‚æ•°è§£æå¤±è´¥: {str(e)}"},
                    "success": False,
                    "latency": tool_call_end - tool_call_start
                })
            except Exception as e:
                error_count += 1
                tool_call_end = time.time()
                error_msg = f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
                tool_output_content += error_msg
                tool_call_history.append({
                    "turn": turn,
                    "tool_name": func_name,
                    "tool_args": arguments,
                    "result": {"error": str(e)},
                    "success": False,
                    "latency": tool_call_end - tool_call_start
                })
        
        if len(tool_output_content) > 4000:
            print(f"    [Agent è­¦å‘Š] å·¥å…·è¾“å‡ºè¿‡é•¿ ({len(tool_output_content)} chars)ï¼Œå°†è¢«æˆªæ–­ã€‚")
            tool_output_content = tool_output_content[:4000] + "\n... [TRUNCATED] ..."
        messages.append({"role": "tool", "content": tool_output_content})
        full_trajectory += f"--- TOOL OUTPUT ---\n{tool_output_content}\n\n"
    
    print(f"    [Agent è­¦å‘Š] è¾¾åˆ°æœ€å¤§ {max_turns} è½®æ¬¡ï¼Œå¼ºè¡Œç»ˆæ­¢ã€‚")
    return final_response, full_trajectory, tool_call_history, error_count


# ==========================================================
# 4. APIæ¨¡å‹ Agent å¾ªç¯
# ==========================================================
def run_agent_loop_api(
    user_question: str,
    model: APIModelWrapper,
    system_prompt: str,
    function_mapper: dict,
    max_turns: int,
    max_tokens: int,
    temperature: float
) -> Tuple[str, str, List[Dict], int]:
    """è¿è¡ŒAPIæ¨¡å‹çš„Agentå¾ªç¯"""
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_question}]
    final_response = ""
    full_trajectory = f"--- USER ---\n{user_question}\n\n"
    tool_call_history = []
    error_count = 0
    
    for turn in range(max_turns):
        print(f"    [Agent] æ­£åœ¨è¿›è¡Œç¬¬ {turn + 1}/{max_turns} æ¨ç†...")
        try:
            response = model.chat(
                messages,
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            messages.append({"role": "assistant", "content": response})
            final_response = response
            full_trajectory += f"--- ASSISTANT (æ€è€ƒä¸å·¥å…·è°ƒç”¨) ---\n{response}\n\n"
            print(f"    [Agent æ¨¡å‹å›å¤] {response[:150]}...")
            
            tool_calls = extract_tool_calls(response)
            
            if not tool_calls:
                final_answer = extract_boxed_answer(response)[0]
                if final_answer:
                    print("    [Agent] æµç¨‹ç»“æŸ (å·²æå–æœ€ç»ˆç­”æ¡ˆ)ã€‚")
                    break
            continue
            
            tool_outputs = []
            for tool_call in tool_calls:
                tool_call_start = time.time()
                try:
                    func = function_mapper.get(tool_call["name"])
                    if func:
                        result = func(**tool_call["args"])
                        tool_call_end = time.time()
                        tool_call_history.append({
                            "turn": turn,
                            "tool_name": tool_call["name"],
                            "tool_args": tool_call["args"],
                            "result": result,
                            "success": True,
                            "latency": tool_call_end - tool_call_start
                        })
                        tool_outputs.append(json.dumps(result, ensure_ascii=False))
                    else:
                        error_count += 1
                        tool_call_history.append({
                            "turn": turn,
                            "tool_name": tool_call["name"],
                            "tool_args": tool_call["args"],
                            "result": {"error": f"å·¥å…· '{tool_call['name']}' ä¸å­˜åœ¨"},
                            "success": False,
                            "latency": 0
                        })
                        tool_outputs.append(f"é”™è¯¯: å·¥å…· '{tool_call['name']}' ä¸å­˜åœ¨")
                except Exception as e:
                    error_count += 1
                    tool_call_end = time.time()
                    tool_call_history.append({
                        "turn": turn,
                        "tool_name": tool_call["name"],
                        "tool_args": tool_call["args"],
                        "result": {"error": str(e)},
                        "success": False,
                        "latency": tool_call_end - tool_call_start
                    })
                    tool_outputs.append(f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}")
            
            if tool_outputs:
                tool_output_str = "\n".join(tool_outputs)
                if len(tool_output_str) > 4000:
                    tool_output_str = tool_output_str[:4000] + "...(å·²æˆªæ–­)"
                messages.append({
                    "role": "user",
                    "content": f"å·¥å…·è°ƒç”¨ç»“æœ:\n{tool_output_str}\n\nè¯·æ ¹æ®å·¥å…·è°ƒç”¨ç»“æœç»§ç»­å¤„ç†ä»»åŠ¡ï¼Œæˆ–ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚"
                })
                full_trajectory += f"--- TOOL OUTPUT ---\n{tool_output_str}\n\n"
            
            final_answer = extract_boxed_answer(response)[0]
            if final_answer:
                break
        
        except Exception as e:
            print(f"    è½®æ¬¡ {turn + 1} å‘ç”Ÿé”™è¯¯: {e}")
            error_count += 1
            break
    
    return final_response, full_trajectory, tool_call_history, error_count


# ==========================================================
# 5. ä¸»è¯„ä¼°å‡½æ•°
# ==========================================================
def main_evaluation(args):
    """ä¸»è¯„ä¼°å‡½æ•°"""
    # åŠ è½½å·¥å…·å®šä¹‰
    try:
        print("--- æ­£åœ¨åŠ è½½å·¥å…·å®šä¹‰å’Œæ ‘çŠ¶ç»“æ„ ---")
        if not os.path.exists(args.tool_tree_path):
            raise FileNotFoundError(f"Missing: {args.tool_tree_path}")
        if not os.path.exists(args.tool_desc_path):
            raise FileNotFoundError(f"Missing: {args.tool_desc_path}")
        with open(args.tool_tree_path, 'r', encoding='utf-8') as f:
            tool_tree = json.load(f)
        with open(args.tool_desc_path, 'r', encoding='utf-8') as f:
            all_tools_description = list(json.load(f).values())
        print(f"--- å·¥å…·å®šä¹‰ ({len(all_tools_description)}ä¸ª) å’Œæ ‘çŠ¶ç»“æ„åŠ è½½æˆåŠŸ ---")
    except Exception as e:
        print(f"åŠ è½½å·¥å…·æ–‡ä»¶æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        sys.exit(1)
    
    # æ„å»ºå·¥å…·æ˜ å°„å™¨
    with open(args.tool_desc_path, 'r', encoding='utf-8') as f:
        tool_desc = json.load(f)
    function_mapper = {
        name: globals()[name]
        for name in tool_desc.keys()
        if name in globals() and callable(globals()[name])
    }
    print(f"--- function_mapper (å…± {len(function_mapper)} ä¸ª) ---")
    
    # æ„å»ºç³»ç»Ÿæç¤ºè¯
    system_prompt = build_final_answer_prompt(tool_tree, all_tools_description)
    
    # åˆå§‹åŒ–æ¨¡å‹
    if args.model_type == "local":
        if not VLLM_AVAILABLE:
            print("é”™è¯¯: éœ€è¦å®‰è£… vLLM æ¥ä½¿ç”¨æœ¬åœ°æ¨¡å‹")
            print("è¯·å®‰è£…: pip install vllm")
            sys.exit(1)
        print(f"--- æ­£åœ¨åŠ è½½ VLLM æ¨¡å‹: {args.model_path} ---")
        try:
            model = LLM(
                model=args.model_path,
                tensor_parallel_size=args.tensor_parallel_size,
                gpu_memory_utilization=args.gpu_memory_utilization,
                max_model_len=args.max_model_len,
                trust_remote_code=True,
                disable_log_stats=True
            )
            tokenizer = model.get_tokenizer()
            sampling_params = SamplingParams(temperature=0.0, max_tokens=args.max_tokens)
            print("--- VLLM æ¨¡å‹å’Œ Tokenizer åŠ è½½æˆåŠŸã€‚")
        except Exception as e:
            print(f"åŠ è½½ vLLM æ¨¡å‹å¤±è´¥: {e}")
            print(traceback.format_exc())
            sys.exit(1)
    else:  # APIæ¨¡å‹
        print(f"--- åˆå§‹åŒ– API æ¨¡å‹: {args.provider}/{args.model_name} ---")
        try:
            model = APIModelWrapper(
                provider=args.provider,
                model_name=args.model_name,
                api_key=args.api_key,
                base_url=args.base_url
            )
            print("--- API æ¨¡å‹åˆå§‹åŒ–æˆåŠŸã€‚")
        except Exception as e:
            print(f"åˆå§‹åŒ– API æ¨¡å‹å¤±è´¥: {e}")
            print(traceback.format_exc())
            sys.exit(1)
    
    # åŠ è½½æ•°æ®é›†
    print("--- å¯åŠ¨è¯„ä¼° (Agentæµç¨‹ + ç›´æ¥å€¼Boxed Answerè¯„ä¼°) ---")
    try:
        with open(args.dataset_file_path, 'r', encoding='utf-8') as f:
            data_list = json.load(f)
        total_count = len(data_list)
        print(f"æˆåŠŸåŠ è½½ {total_count} æ¡æ•°æ®ã€‚")
    except Exception as e: 
        print(f"ã€è‡´å‘½é”™è¯¯ã€‘: åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        return
        
    # å¤„ç†å·²å®Œæˆçš„æ¡ç›®
    processed_prompts = set()
    if os.path.exists(args.output_file_path):
        try:
            with open(args.output_file_path, 'r', encoding='utf-8') as f_in:
                for line in f_in:
                    if line.strip():
                        try:
                            data = json.loads(line)
                            if "question" in data:
                                processed_prompts.add(data["question"])
                        except json.JSONDecodeError:
                            pass
            print(f"--- å·²åŠ è½½ {len(processed_prompts)} ä¸ªå·²å¤„ç†çš„ promptsï¼Œå°†åœ¨æœ¬æ¬¡è¿è¡Œä¸­è·³è¿‡å®ƒä»¬ã€‚---\n")
        except Exception as e:
            print(f"--- è¯»å–ç°æœ‰è¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}. å°†ä»å¤´å¼€å§‹å¤„ç†ã€‚ ---")
            
    results_log = [] 
    success_count_this_run = 0
    
    with open(args.output_file_path, 'a', encoding='utf-8') as f_out:
        for i, item in enumerate(data_list):
            prompt = item.get("question")
            ground_truth_code = item.get("code")
            task_start_time = time.time()
            
            if prompt in processed_prompts:
                continue
                
            print(f"\n--- æ­£åœ¨è¯„ä¼°ç¬¬ {i+1}/{total_count} é¡¹ (æ–°é¡¹ç›®) ---")
            
            if not prompt or not ground_truth_code: 
                print("    [è·³è¿‡] æ•°æ®ç¼ºå¤± ('question' or 'code')ã€‚")
                continue
                
            print(f"    [Prompt] {prompt[:100]}...")
            
            log_entry = { 
                "task_id": i,
                "question": prompt,
                "chain_category": item.get("chain_category", "unknown"),
                "tool_count": item.get("tool_count", 0),
                        "is_correct": False, 
                        "eval_score": 0 
                        }
            
            # æ‰§è¡ŒåŸºå‡†ä»£ç 
            success, ground_truth_answer, err = execute_simplified_code(ground_truth_code, function_mapper)
            
            if not success:
                print(f"    âŒ åŸºå‡†ä»£ç æ‰§è¡Œå¤±è´¥: {err}")
                log_entry["ground_truth_error"] = str(err)
                f_out.write(json.dumps(log_entry, ensure_ascii=False, default=_numpy_converter) + '\n')
                f_out.flush()
                processed_prompts.add(prompt)
                continue
                
            log_entry["ground_truth"] = ground_truth_answer
            print(f"    âœ… åŸºå‡†ç­”æ¡ˆ: {ground_truth_answer}")
            
            # è¿è¡ŒAgentå¾ªç¯
            if args.model_type == "local":
                final_response, trajectory, tool_call_history, error_count = run_agent_loop_local(
                    prompt, model, tokenizer, sampling_params, system_prompt, function_mapper, args.max_turns
                )
            else:
                final_response, trajectory, tool_call_history, error_count = run_agent_loop_api(
                    prompt, model, system_prompt, function_mapper, args.max_turns, args.max_tokens, args.temperature
                )
            
            task_end_time = time.time()
            latency = task_end_time - task_start_time
            
            log_entry["model_trajectory"] = trajectory
            log_entry["tool_call_history"] = tool_call_history
            log_entry["error_count"] = error_count
            log_entry["total_turns"] = trajectory.count("--- ASSISTANT")
            log_entry["latency"] = latency
            log_entry["conversation_history"] = []
            
            # æå–æ¨¡å‹ç­”æ¡ˆ
            model_answer, err = extract_boxed_answer(final_response)
            
            if err:
                print(f"    âŒ æ¨¡å‹ç­”æ¡ˆæå–å¤±è´¥: {err}")
                log_entry["model_error"] = err
                log_entry["final_answer"] = None
                log_entry["abandoned"] = True
                f_out.write(json.dumps(log_entry, ensure_ascii=False, default=_numpy_converter) + '\n')
                f_out.flush()
                processed_prompts.add(prompt)
                continue
                
            log_entry["final_answer"] = model_answer
            log_entry["abandoned"] = False
            print(f"    ğŸ¤– æ¨¡å‹ç­”æ¡ˆ: {model_answer}")
            
            # æ¯”è¾ƒç­”æ¡ˆ
            is_correct = compare_answers(model_answer, ground_truth_answer)
            log_entry["is_correct"] = is_correct
            
            if is_correct:
                log_entry["eval_score"] = 1
                success_count_this_run += 1
                print("    ğŸ‘ ç»“æœ: æ­£ç¡®")
            else:
                log_entry["eval_score"] = 0
                print("    ğŸ‘ ç»“æœ: é”™è¯¯")
                
            json_str = json.dumps(log_entry, ensure_ascii=False, default=_numpy_converter)
            f_out.write(json_str + '\n')
            f_out.flush()
            processed_prompts.add(prompt)
            results_log.append(log_entry) 
            
            # APIæ¨¡å‹é™æµ
            if args.model_type == "api":
                time.sleep(0.5)
    
    # ç»Ÿè®¡ç»“æœ
    total_processed_in_file = len(processed_prompts)
    processed_this_run = len(results_log)
    total_correct_in_file = 0
    
    if os.path.exists(args.output_file_path):
        with open(args.output_file_path, 'r', encoding='utf-8') as f_final:
            for line in f_final:
                if line.strip():
                    try:
                        final_data = json.loads(line)
                        if final_data.get("is_correct") is True:
                            total_correct_in_file += 1
                    except:
                        pass
                    
    print("\n" + "="*50)
    print("--- è¯„ä¼°æ€»ç»“ ---")
    print(f"æœ¬æ¬¡è¿è¡Œå¤„ç†æ¡ç›®:  {processed_this_run}")
    print(f"  - æœ¬æ¬¡è¿è¡Œæ­£ç¡®:  {success_count_this_run}")
    print("-" * 20)
    print(f"è¾“å…¥æ–‡ä»¶æ€»æ¡ç›®:      {total_count}")
    print(f"è¾“å‡ºæ–‡ä»¶ä¸­æ€»å·²å¤„ç†:  {total_processed_in_file}")
    print(f"è¾“å‡ºæ–‡ä»¶ä¸­æ€»æ­£ç¡®:    {total_correct_in_file}")
    
    if total_processed_in_file > 0:
        overall_accuracy = (total_correct_in_file / total_processed_in_file) * 100
        print(f"åŸºäºå·²å¤„ç†æ¡ç›®çš„æ€»æˆåŠŸç‡: {overall_accuracy:.2f}%")
        
    print(f"è¯¦ç»†æ—¥å¿—å·²è¿½åŠ åˆ°: {args.output_file_path}")
    print("="*50)


# ==========================================================
# 6. è¯„ä¼°æŒ‡æ ‡è®¡ç®—ï¼ˆæ¥è‡ª calculate_metrics.pyï¼‰
# ==========================================================
def normalize_string(s: str) -> str:
    """è§„èŒƒåŒ–å­—ç¬¦ä¸²ï¼ˆç§»é™¤ç©ºæ ¼ã€æ ‡ç‚¹ç­‰ï¼‰"""
    if not isinstance(s, str):
        s = str(s)
    s = s.strip()
    s = s.replace(',', '')
    return s


def try_convert_type(value: Any, target_type: type) -> Optional[Any]:
    """å°è¯•ç±»å‹è½¬æ¢"""
    try:
        if target_type == float:
            return float(value)
        elif target_type == int:
            return int(float(value))
        elif target_type == str:
            return str(value)
    except:
        return None
    return None


def is_answer_match(model_answer: Any, ground_truth_answer: Any, tolerance: float = 1e-6) -> bool:
    """åˆ¤æ–­ç­”æ¡ˆæ˜¯å¦åŒ¹é…ï¼ˆå®Œå…¨åŸºäºè§„åˆ™ï¼Œä¸ä½¿ç”¨LLMï¼‰"""
    if model_answer is None:
        return False
    
    if type(model_answer) != type(ground_truth_answer):
        converted = try_convert_type(model_answer, type(ground_truth_answer))
        if converted is None:
            return False
        model_answer = converted
    
    if isinstance(ground_truth_answer, (int, float)):
        if isinstance(model_answer, (int, float)):
            if abs(ground_truth_answer) < 1e-10:
                error = abs(model_answer - ground_truth_answer)
                return error < tolerance
            else:
                relative_error = abs(model_answer - ground_truth_answer) / abs(ground_truth_answer)
                return relative_error < tolerance
    
    elif isinstance(ground_truth_answer, str):
        if isinstance(model_answer, str):
            normalized_model = normalize_string(model_answer)
            normalized_truth = normalize_string(ground_truth_answer)
            return normalized_model == normalized_truth
    
    elif isinstance(ground_truth_answer, (list, tuple)):
        if isinstance(model_answer, (list, tuple)):
            if len(model_answer) != len(ground_truth_answer):
                return False
            model_sorted = sorted(model_answer, key=str)
            truth_sorted = sorted(ground_truth_answer, key=str)
            return all(is_answer_match(m, t, tolerance) for m, t in zip(model_sorted, truth_sorted))
    
    elif isinstance(ground_truth_answer, dict):
        if isinstance(model_answer, dict):
            if set(model_answer.keys()) != set(ground_truth_answer.keys()):
                return False
            return all(is_answer_match(model_answer[k], ground_truth_answer[k], tolerance) 
                      for k in ground_truth_answer.keys())
    
    else:
        return model_answer == ground_truth_answer
    
    return False


def has_tool_call(tool_call_history: List[Dict]) -> bool:
    """åˆ¤æ–­æ˜¯å¦è‡³å°‘è°ƒç”¨äº†ä¸€ä¸ªå·¥å…·"""
    return len(tool_call_history) > 0


def are_all_tool_calls_successful(tool_call_history: List[Dict]) -> bool:
    """åˆ¤æ–­æ‰€æœ‰å·¥å…·è°ƒç”¨æ˜¯å¦æˆåŠŸ"""
    if len(tool_call_history) == 0:
        return False
    
    for call in tool_call_history:
        if not call.get('success', False):
            return False
        
        result = call.get('result')
        if result is None:
            return False
        
        if isinstance(result, dict):
            error_keys = ['error', 'Error', 'ERROR', 'exception', 'Exception', 'timeout', 'Timeout']
            for key in error_keys:
                if key in result:
                    return False
    
    return True


def count_tool_call_errors(tool_call_history: List[Dict]) -> int:
    """ç»Ÿè®¡å·¥å…·è°ƒç”¨å¤±è´¥æ¬¡æ•°"""
    error_count = 0
    for call in tool_call_history:
        if not call.get('success', False):
            error_count += 1
    return error_count


def is_abandoned(result: Dict, max_turns: int = 10) -> bool:
    """åˆ¤æ–­ä»»åŠ¡æ˜¯å¦è¢«æå‰æ”¾å¼ƒ"""
    total_turns = result.get('total_turns', 0)
    if total_turns >= max_turns:
        return False
    
    final_answer = result.get('final_answer')
    if final_answer is None:
        return True
    
    return False


def calculate_metrics(task_results: List[Dict], max_turns: int = 10) -> Dict[str, float]:
    """è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡"""
    total_tasks = len(task_results)
    
    if total_tasks == 0:
        return {
            "tsr": 0.0, "faa": 0.0, "memory_cheating_rate": 0.0, "cer": 0.0,
            "ar": 0.0, "avg_eep": 0.0, "frr": 0.0, "avg_lc": 0.0
        }
    
    tsr_count = 0
    faa_count = 0
    cer_successful_tool_calls = 0
    cer_error_count = 0
    ar_count = 0
    total_eep = 0
    frr_with_errors = 0
    frr_resolved = 0
    total_latency = 0
    
    for result in task_results:
        final_answer = result.get('final_answer')
        ground_truth = result.get('ground_truth')
        tool_call_history = result.get('tool_call_history', [])
        error_count = result.get('error_count', 0)
        latency = result.get('latency', 0.0)
        
        answer_match = is_answer_match(final_answer, ground_truth)
        has_tool = has_tool_call(tool_call_history)
        if answer_match and has_tool:
            tsr_count += 1
        
        if answer_match:
            faa_count += 1
        
        all_tools_successful = are_all_tool_calls_successful(tool_call_history)
        if all_tools_successful and len(tool_call_history) > 0:
            cer_successful_tool_calls += 1
            if not answer_match:
                cer_error_count += 1
        
        if is_abandoned(result, max_turns):
            ar_count += 1
        
        if error_count > 0:
            total_eep += error_count
        else:
            total_eep += count_tool_call_errors(tool_call_history)
        
        task_error_count = error_count if error_count > 0 else count_tool_call_errors(tool_call_history)
        if task_error_count > 0:
            frr_with_errors += 1
            if answer_match and has_tool:
                frr_resolved += 1
        
        total_latency += latency
    
    metrics = {
        "tsr": tsr_count / total_tasks,
        "faa": faa_count / total_tasks,
        "memory_cheating_rate": (faa_count - tsr_count) / total_tasks,
        "cer": cer_error_count / cer_successful_tool_calls if cer_successful_tool_calls > 0 else 0.0,
        "ar": ar_count / total_tasks,
        "avg_eep": total_eep / total_tasks,
        "frr": frr_resolved / frr_with_errors if frr_with_errors > 0 else 1.0,
        "avg_lc": total_latency / total_tasks
    }
    
    return metrics


def calculate_metrics_by_category(task_results: List[Dict], max_turns: int = 10) -> Dict[str, Dict[str, float]]:
    """æŒ‰ä»»åŠ¡å¤æ‚åº¦åˆ†ç±»è®¡ç®—æŒ‡æ ‡"""
    from collections import defaultdict
    by_category = defaultdict(list)
    for result in task_results:
        category = result.get('chain_category', 'unknown')
        by_category[category].append(result)
    
    metrics_by_category = {}
    for category, results in by_category.items():
        metrics_by_category[category] = calculate_metrics(results, max_turns)
    
    return metrics_by_category


def calculate_metrics_by_tool_count(task_results: List[Dict], max_turns: int = 10) -> Dict[str, Dict[str, float]]:
    """æŒ‰å·¥å…·é“¾é•¿åº¦åˆ†ç±»è®¡ç®—æŒ‡æ ‡"""
    from collections import defaultdict
    by_tool_count = defaultdict(list)
    for result in task_results:
        tool_count = result.get('tool_count', 0)
        by_tool_count[tool_count].append(result)
    
    metrics_by_tool_count = {}
    for tool_count, results in sorted(by_tool_count.items()):
        metrics_by_tool_count[f"tool_count_{tool_count}"] = calculate_metrics(results, max_turns)
        metrics_by_tool_count[f"tool_count_{tool_count}"]["task_count"] = len(results)
    
    return metrics_by_tool_count


def generate_metrics_report(task_results: List[Dict], max_turns: int = 10) -> Dict[str, Any]:
    """ç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æŠ¥å‘Š"""
    overall_metrics = calculate_metrics(task_results, max_turns)
    metrics_by_category = calculate_metrics_by_category(task_results, max_turns)
    metrics_by_tool_count = calculate_metrics_by_tool_count(task_results, max_turns)
    
    from collections import defaultdict
    by_category = defaultdict(int)
    by_tool_count = defaultdict(int)
    for result in task_results:
        by_category[result.get('chain_category', 'unknown')] += 1
        by_tool_count[result.get('tool_count', 0)] += 1
    
    report = {
        "summary": {
            "total_tasks": len(task_results),
            "max_turns": max_turns
        },
        "overall_metrics": overall_metrics,
        "metrics_by_category": metrics_by_category,
        "metrics_by_tool_count": metrics_by_tool_count,
        "detailed_breakdown": {
            "category_statistics": dict(by_category),
            "tool_count_statistics": dict(by_tool_count)
        }
    }
    
    return report


def calculate_metrics_main(args):
    """è®¡ç®—æŒ‡æ ‡çš„ä¸»å‡½æ•°"""
    task_results = []
    with open(args.results_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                task_results.append(json.loads(line))
    
    report = generate_metrics_report(task_results, args.max_turns)
    
    with open(args.output_file, 'w', encoding='utf-8') as f:
        if args.pretty:
            json.dump(report, f, ensure_ascii=False, indent=2)
        else:
            json.dump(report, f, ensure_ascii=False)
    
    print("=" * 60)
    print("FinBoom è¯„ä¼°æŒ‡æ ‡æŠ¥å‘Š")
    print("=" * 60)
    print(f"\næ€»ä»»åŠ¡æ•°: {report['summary']['total_tasks']}")
    print(f"æœ€å¤§è½®æ¬¡: {report['summary']['max_turns']}")
    print("\næ€»ä½“æŒ‡æ ‡:")
    print("-" * 60)
    for key, value in report['overall_metrics'].items():
        if isinstance(value, float):
            print(f"  {key.upper():25s}: {value:.4f}")
        else:
            print(f"  {key.upper():25s}: {value}")
    
    print("\næŒ‰ä»»åŠ¡å¤æ‚åº¦åˆ†ç±»:")
    print("-" * 60)
    for category, metrics in report['metrics_by_category'].items():
        print(f"\n  {category.upper()}:")
        for key, value in metrics.items():
            if isinstance(value, float):
                print(f"    {key.upper():25s}: {value:.4f}")
            else:
                print(f"    {key.upper():25s}: {value}")
    
    print("\n" + "=" * 60)
    print(f"è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output_file}")
    print("=" * 60)


# ==========================================================
# 7. ç»“æœå¯¹æ¯”ï¼ˆæ¥è‡ª compare_results.pyï¼‰
# ==========================================================
def compare_results_main(args):
    """å¯¹æ¯”å¤šä¸ªæ¨¡å‹ç»“æœçš„ä¸»å‡½æ•°"""
    try:
        import pandas as pd
    except ImportError:
        print("é”™è¯¯: éœ€è¦å®‰è£… pandas: pip install pandas")
        sys.exit(1)
    
    from pathlib import Path
    
    all_metrics = []
    for metrics_file in args.metrics_files:
        with open(metrics_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        model_name = Path(metrics_file).stem.replace('metrics_', '')
        metrics = {
            'Model': model_name,
            **data['overall_metrics']
        }
        all_metrics.append(metrics)
    
    df = pd.DataFrame(all_metrics)
    
    numeric_columns = ['tsr', 'faa', 'memory_cheating_rate', 'cer', 'ar', 'avg_eep', 'frr', 'avg_lc']
    for col in numeric_columns:
        if col in df.columns:
            df[col] = df[col].apply(lambda x: f"{x:.4f}" if isinstance(x, (int, float)) else x)
    
    if args.format == 'markdown':
        output = generate_markdown_comparison(df, all_metrics)
    elif args.format == 'csv':
        output = df.to_csv(index=False)
    elif args.format == 'json':
        output = json.dumps(all_metrics, indent=2, ensure_ascii=False)
    else:
        output = df.to_string(index=False)
    
    print(output)
    
    if args.output_file:
        with open(args.output_file, 'w', encoding='utf-8') as f:
            f.write(output)
        print(f"\nå¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output_file}")


def generate_markdown_comparison(df, all_metrics: List[Dict]) -> str:
    """ç”ŸæˆMarkdownæ ¼å¼çš„å¯¹æ¯”æŠ¥å‘Š"""
    report = []
    report.append("# FinBoom æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š\n")
    report.append("## æ€»ä½“æŒ‡æ ‡å¯¹æ¯”\n")
    report.append("| æ¨¡å‹ | TSR | FAA | è®°å¿†ä½œå¼Šç‡ | CER | AR | Avg. EEP | FRR | Avg. LC |")
    report.append("|------|-----|-----|-----------|-----|-----|----------|-----|---------|")
    
    for _, row in df.iterrows():
        report.append(
            f"| {row['Model']} | {row.get('tsr', 'N/A')} | {row.get('faa', 'N/A')} | "
            f"{row.get('memory_cheating_rate', 'N/A')} | {row.get('cer', 'N/A')} | "
            f"{row.get('ar', 'N/A')} | {row.get('avg_eep', 'N/A')} | "
            f"{row.get('frr', 'N/A')} | {row.get('avg_lc', 'N/A')} |"
        )
    
    report.append("")
    return "\n".join(report)


# ==========================================================
# 8. Benchmarkå¯¹æ¯”ï¼ˆæ¥è‡ª benchmark_comparison.pyï¼‰
# ==========================================================
def benchmark_comparison_main(args):
    """Benchmarkå¯¹æ¯”çš„ä¸»å‡½æ•°"""
    try:
        import pandas as pd
    except ImportError:
        print("é”™è¯¯: éœ€è¦å®‰è£… pandas: pip install pandas")
        sys.exit(1)
    
    with open(args.finboom_metrics, 'r', encoding='utf-8') as f:
        finboom_metrics = json.load(f)
    
    other_benchmarks = []
    for item in args.other_benchmarks:
        if ':' in item:
            name, file_path = item.split(':', 1)
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            other_benchmarks.append({
                "benchmark_name": name,
                "metrics": data
            })
    
    finboom_overall = finboom_metrics.get("overall_metrics", {})
    
    comparison_table = [{
        "Benchmark": "FinBoom",
        "TSR/Accuracy": f"{finboom_overall.get('tsr', 0):.4f}",
        "FAA": f"{finboom_overall.get('faa', 0):.4f}",
        "Memory Cheating Rate": f"{finboom_overall.get('memory_cheating_rate', 0):.4f}",
        "CER": f"{finboom_overall.get('cer', 0):.4f}",
        "AR": f"{finboom_overall.get('ar', 0):.4f}",
        "Avg. EEP": f"{finboom_overall.get('avg_eep', 0):.4f}",
        "FRR": f"{finboom_overall.get('frr', 0):.4f}",
        "Avg. LC": f"{finboom_overall.get('avg_lc', 0):.4f}",
    }]
    
    for other in other_benchmarks:
        other_metrics = other["metrics"]
        comparison_table.append({
            "Benchmark": other["benchmark_name"],
            "TSR/Accuracy": f"{other_metrics.get('accuracy', other_metrics.get('acc', 'N/A'))}",
            "FAA": "N/A",
            "Memory Cheating Rate": "N/A",
            "CER": "N/A",
            "AR": "N/A",
            "Avg. EEP": "N/A",
            "FRR": "N/A",
            "Avg. LC": "N/A",
        })
    
    report = {
        "comparison_table": comparison_table,
        "finboom_metrics": finboom_overall
    }
    
    with open(args.output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print("=" * 80)
    print("FinBoom Benchmarkå¯¹æ¯”åˆ†ææŠ¥å‘Š")
    print("=" * 80)
    print("\nå¯¹æ¯”è¡¨:")
    print("-" * 80)
    df = pd.DataFrame(comparison_table)
    print(df.to_string(index=False))
    print(f"\n\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output_file}")
    print("=" * 80)


# ==========================================================
# 9. å‚æ•°è§£æ
# ==========================================================
def parse_args():
    parser = argparse.ArgumentParser(
        description="FinBoom ç»Ÿä¸€è¯„ä¼°å·¥å…·ï¼ˆæ”¯æŒè¯„ä¼°ã€æŒ‡æ ‡è®¡ç®—ã€ç»“æœå¯¹æ¯”ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # è¿è¡Œè¯„ä¼°ï¼ˆæœ¬åœ°æ¨¡å‹ï¼‰
  python eval.py evaluate --model-type local --model-path /path/to/model ...
  
  # è¿è¡Œè¯„ä¼°ï¼ˆAPIæ¨¡å‹ï¼‰
  python eval.py evaluate --model-type api --provider openai --model-name gpt-4 ...
  
  # è®¡ç®—æŒ‡æ ‡
  python eval.py calculate-metrics --results-file results.jsonl --output-file metrics.json
  
  # å¯¹æ¯”å¤šä¸ªæ¨¡å‹
  python eval.py compare --metrics-files metrics1.json metrics2.json --output-file comparison.md
  
  # Benchmarkå¯¹æ¯”
  python eval.py benchmark-comparison --finboom-metrics metrics.json --other-benchmarks name1:file1.json name2:file2.json --output-file comparison.json
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # è¯„ä¼°å‘½ä»¤
    eval_parser = subparsers.add_parser('evaluate', help='è¿è¡Œæ¨¡å‹è¯„ä¼°')
    eval_parser.add_argument(
        "--model-type",
        type=str,
        required=True,
        choices=["local", "api"],
        help="æ¨¡å‹ç±»å‹: 'local' ä½¿ç”¨æœ¬åœ°vLLMæ¨¡å‹, 'api' ä½¿ç”¨APIæ¨¡å‹"
    )
    
    # æœ¬åœ°æ¨¡å‹å‚æ•°
    eval_parser.add_argument(
        "--model-path",
        type=str,
        default=None,
        help="æœ¬åœ°æ¨¡å‹çš„è·¯å¾„ï¼ˆä»…æœ¬åœ°æ¨¡å‹éœ€è¦ï¼‰"
    )
    eval_parser.add_argument(
        "--tensor-parallel-size",
        type=int,
        default=1,
        help="å¼ é‡å¹¶è¡Œå¤§å°ï¼ˆä»…æœ¬åœ°æ¨¡å‹éœ€è¦ï¼‰"
    )
    eval_parser.add_argument(
        "--gpu-memory-utilization",
        type=float,
        default=0.9,
        help="GPUå†…å­˜åˆ©ç”¨ç‡ï¼ˆä»…æœ¬åœ°æ¨¡å‹éœ€è¦ï¼‰"
    )
    eval_parser.add_argument(
        "--max-model-len",
        type=int,
        default=8192,
        help="æ¨¡å‹æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆä»…æœ¬åœ°æ¨¡å‹éœ€è¦ï¼‰"
    )
    
    # APIæ¨¡å‹å‚æ•°
    eval_parser.add_argument(
        "--provider",
        type=str,
        default=None,
        choices=["openai", "anthropic", "google", "qwen"],
        help="APIæä¾›å•†ï¼ˆä»…APIæ¨¡å‹éœ€è¦ï¼‰"
    )
    eval_parser.add_argument(
        "--model-name",
        type=str,
        default=None,
        help="APIæ¨¡å‹åç§°ï¼ˆä»…APIæ¨¡å‹éœ€è¦ï¼‰"
    )
    eval_parser.add_argument(
        "--api-key",
        type=str,
        default=None,
        help="APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œä¹Ÿå¯é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®ï¼‰"
    )
    eval_parser.add_argument(
        "--base-url",
        type=str,
        default=None,
        help="APIåŸºç¡€URLï¼ˆå¯é€‰ï¼‰"
    )
    eval_parser.add_argument(
        "--temperature",
        type=float,
        default=0.0,
        help="é‡‡æ ·æ¸©åº¦ï¼ˆä»…APIæ¨¡å‹ï¼Œé»˜è®¤0.0ï¼‰"
    )
    
    # é€šç”¨å‚æ•°
    eval_parser.add_argument(
        "--max-tokens",
        type=int,
        default=2048,
        help="æœ€å¤§ç”Ÿæˆtokenæ•°"
    )
    eval_parser.add_argument(
        "--max-turns",
        type=int,
        default=10,
        help="Agentå¾ªç¯çš„æœ€å¤§æ¨ç†è½®æ¬¡"
    )
    eval_parser.add_argument(
        "--output-file-path",
        type=str,
        required=True,
        help="è¯„ä¼°ç»“æœçš„è¾“å‡ºæ–‡ä»¶è·¯å¾„"
    )
    eval_parser.add_argument(
        "--dataset-file-path",
        type=str,
        required=True,
        help="æ•°æ®é›†è·¯å¾„"
    )
    eval_parser.add_argument(
        "--tool-tree-path",
        type=str,
        required=True,
        help="å·¥å…·æ ‘è·¯å¾„"
    )
    eval_parser.add_argument(
        "--tool-desc-path",
        type=str,
        required=True,
        help="å·¥å…·å®šä¹‰è·¯å¾„"
    )
    
    # è®¡ç®—æŒ‡æ ‡å‘½ä»¤
    metrics_parser = subparsers.add_parser('calculate-metrics', help='è®¡ç®—è¯„ä¼°æŒ‡æ ‡')
    metrics_parser.add_argument(
        "--results-file",
        type=str,
        required=True,
        help="ä»»åŠ¡ç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆJSONLæ ¼å¼ï¼‰"
    )
    metrics_parser.add_argument(
        "--output-file",
        type=str,
        required=True,
        help="è¯„ä¼°æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰"
    )
    metrics_parser.add_argument(
        "--max-turns",
        type=int,
        default=10,
        help="æœ€å¤§æ¨ç†è½®æ¬¡ï¼ˆé»˜è®¤ï¼š10ï¼‰"
    )
    metrics_parser.add_argument(
        "--pretty",
        action="store_true",
        help="ç¾åŒ–JSONè¾“å‡º"
    )
    
    # å¯¹æ¯”ç»“æœå‘½ä»¤
    compare_parser = subparsers.add_parser('compare', help='å¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„è¯„ä¼°ç»“æœ')
    compare_parser.add_argument(
        "--metrics-files",
        type=str,
        nargs='+',
        required=True,
        help="æŒ‡æ ‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆå¯å¤šä¸ªï¼‰"
    )
    compare_parser.add_argument(
        "--output-file",
        type=str,
        default=None,
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰"
    )
    compare_parser.add_argument(
        "--format",
        type=str,
        choices=['markdown', 'csv', 'json', 'table'],
        default='markdown',
        help="è¾“å‡ºæ ¼å¼ï¼ˆé»˜è®¤ï¼šmarkdownï¼‰"
    )
    
    # Benchmarkå¯¹æ¯”å‘½ä»¤
    benchmark_parser = subparsers.add_parser('benchmark-comparison', help='å¯¹æ¯”FinBoomä¸å…¶ä»–benchmark')
    benchmark_parser.add_argument(
        "--finboom-metrics",
        type=str,
        required=True,
        help="FinBoomæŒ‡æ ‡æ–‡ä»¶è·¯å¾„"
    )
    benchmark_parser.add_argument(
        "--other-benchmarks",
        type=str,
        nargs='+',
        required=True,
        help="å…¶ä»–benchmarkæŒ‡æ ‡æ–‡ä»¶è·¯å¾„ï¼ˆæ ¼å¼ï¼šname:file_pathï¼‰"
    )
    benchmark_parser.add_argument(
        "--output-file",
        type=str,
        required=True,
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„"
    )
    
    args = parser.parse_args()
    
    # éªŒè¯è¯„ä¼°å‘½ä»¤çš„å‚æ•°
    if args.command == 'evaluate':
        if args.model_type == "local":
            if not args.model_path:
                eval_parser.error("--model-path æ˜¯å¿…éœ€çš„ï¼ˆå½“ä½¿ç”¨æœ¬åœ°æ¨¡å‹æ—¶ï¼‰")
        else:  # APIæ¨¡å‹
            if not args.provider:
                eval_parser.error("--provider æ˜¯å¿…éœ€çš„ï¼ˆå½“ä½¿ç”¨APIæ¨¡å‹æ—¶ï¼‰")
            if not args.model_name:
                eval_parser.error("--model-name æ˜¯å¿…éœ€çš„ï¼ˆå½“ä½¿ç”¨APIæ¨¡å‹æ—¶ï¼‰")
    
    return args


# ==========================================================
# 10. ä¸»å…¥å£
# ==========================================================
if __name__ == "__main__":
    args = parse_args()
    
    if args.command == 'evaluate':
        main_evaluation(args)
    elif args.command == 'calculate-metrics':
        calculate_metrics_main(args)
    elif args.command == 'compare':
        compare_results_main(args)
    elif args.command == 'benchmark-comparison':
        benchmark_comparison_main(args)
    else:
        print("é”™è¯¯: è¯·æŒ‡å®šä¸€ä¸ªå‘½ä»¤ (evaluate, calculate-metrics, compare, benchmark-comparison)")
        print("ä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯")
        sys.exit(1)
