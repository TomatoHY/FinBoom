#!/usr/bin/env python
# -*- coding:utf-8 -*-
import json
import os
import re
import sys
import math
import argparse
import traceback
from typing import List, Dict, Any, Optional, Set, Tuple, Union
import numpy as np
# = a========================================================
# 0. VLLM å’Œ Tools å¯¼å…¥
# ==========================================================
try:
    from vllm import LLM, SamplingParams
except ImportError:
    print("æœªæ‰¾åˆ° 'vllm' åº“ã€‚")
    print("è¯·å…ˆå®‰è£… vLLM: pip install vllm")
    sys.exit(1)
try:
    from data.tool_library import *
except ImportError:
    print("æ‰¾ä¸åˆ° tool_library, è¯·ç¡®ä¿æ‚¨åœ¨ 'finbench' æ ¹ç›®å½•ä¸‹è¿è¡Œ")
    sys.exit(1)
# ==========================================================
# 1. é…ç½®
# ==========================================================
def parse_args():
    parser = argparse.ArgumentParser(description="è¿è¡Œ FinBench è¯„ä¼°")
    parser.add_argument(
        "--model-path", 
        type=str, 
        required=True,
        help="VLLMæ¨¡å‹çš„æœ¬åœ°æˆ–Hugging Faceè·¯å¾„ã€‚"
    )
    parser.add_argument(
        "--tensor-parallel-size", 
        type=int, 
        required=True,
        help="æ¨¡å‹è·¨è¶Šçš„GPUæ•°é‡ï¼ˆå¼ é‡å¹¶è¡Œå¤§å°ï¼‰ã€‚"
    )
    parser.add_argument(
        "--gpu-memory-utilization", 
        type=float, 
        required=True,
        help="åˆ†é…ç»™æ¨¡å‹çš„GPUå†…å­˜ç™¾åˆ†æ¯”ã€‚"
    )
    parser.add_argument(
        "--max-model-len",
        type=int,
        required=True,  
        help="VLLMæ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆmax_model_lenï¼‰ã€‚"
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        required=True,
        help="æ¨¡å‹ç”Ÿæˆæ¯ä¸ªå“åº”çš„æœ€å¤§tokenæ•°ï¼ˆmax_tokensï¼‰ã€‚"
    )
    parser.add_argument(
        "--max-turns",
        type=int,
        required=True,
        help="Agent å¾ªç¯çš„æœ€å¤§æ¨ç†è½®æ¬¡ã€‚"
    )
    parser.add_argument(
        "--output-file-path", 
        type=str, 
        required=True,
        help="è¯„ä¼°ç»“æœçš„è¾“å‡ºæ–‡ä»¶è·¯å¾„ã€‚"
    )
    parser.add_argument(
        "--dataset-file-path", 
        type=str, 
        required=True, 
        help="æ•°æ®é›†è·¯å¾„ã€‚"
        )
    parser.add_argument(
        "--tool-tree-path", 
        type=str, 
        required=True, 
        help="å·¥å…·æ ‘è·¯å¾„ã€‚"
        )
    parser.add_argument(
        "--tool-desc-path", 
        type=str, 
        required=True, 
        help="å·¥å…·å®šä¹‰è·¯å¾„ã€‚"
        )
    return parser.parse_args()

args = parse_args()
MODEL_PATH = args.model_path
OUTPUT_FILE_PATH = args.output_file_path
DATASET_FILE_PATH = args.dataset_file_path
TOOL_TREE_PATH = args.tool_tree_path
TOOL_DESC_PATH = args.tool_desc_path
TENSOR_PARALLEL_SIZE = args.tensor_parallel_size
GPU_MEM_UTILIZATION = args.gpu_memory_utilization
MAX_MODEL_LEN = args.max_model_len
MAX_TOKENS = args.max_tokens
MAX_TURNS = args.max_turns

# ==========================================================
# 1.5 åŠ è½½å·¥å…·å®šä¹‰å’Œæ ‘çŠ¶ç»“æ„
# ==========================================================
try:
    print("--- æ­£åœ¨åŠ è½½å·¥å…·å®šä¹‰å’Œæ ‘çŠ¶ç»“æ„ ---")
    if not os.path.exists(TOOL_TREE_PATH): raise FileNotFoundError(f"Missing: {TOOL_TREE_PATH}")
    if not os.path.exists(TOOL_DESC_PATH): raise FileNotFoundError(f"Missing: {TOOL_DESC_PATH}")
    with open(TOOL_TREE_PATH, 'r', encoding='utf-8') as f: tool_tree = json.load(f)
    with open(TOOL_DESC_PATH, 'r', encoding='utf-8') as f: all_tools_description = list(json.load(f).values()) 
    print(f"--- å·¥å…·å®šä¹‰ ({len(all_tools_description)}ä¸ª) å’Œæ ‘çŠ¶ç»“æ„åŠ è½½æˆåŠŸ ---")
except Exception as e:
    print(f"åŠ è½½å·¥å…·æ–‡ä»¶æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}"); sys.exit(1)

# ==========================================================
# 2. åˆå§‹åŒ– VLLM æ¨¡å‹
# ==========================================================
model: Optional[LLM] = None
tokenizer: Optional[Any] = None
sampling_params: Optional[SamplingParams] = None
# ==========================================================
# 3. å®šä¹‰å·¥å…·æ˜ å°„å™¨
# ==========================================================
function_mapper = {
        name: globals()[name] 
        for name in json.load(open(TOOL_DESC_PATH, 'r', encoding='utf-8')).keys() 
        if name in globals() and callable(globals()[name])
    }
print(f"--- function_mapper (å…± {len(function_mapper)} ä¸ª) ---")
# ==========================================================
# 4. æ„å»ºæ··åˆæ¨¡å¼ System Prompt
# ==========================================================
def format_tool_tree_for_prompt(tool_tree_dict, indent_level=0):
    markdown_str = ""
    indent = "  " * indent_level
    for key, value in tool_tree_dict.items():
        if key == "_description": continue
        if isinstance(value, dict):
            description = value.get("_description", "")
            markdown_str += f"{indent}- **{key}**: {description}\n"
            if isinstance(value.get("children"), dict):
                markdown_str += format_tool_tree_for_prompt(value["children"], indent_level + 1)
        else:
            markdown_str += f"{indent}- **{key}**\n"
    return markdown_str
# ==========================================================
# 4. æ„å»ºæœ€ç»ˆç­”æ¡ˆæå–æ¨¡å¼ System Prompt
# ==========================================================
def build_final_answer_prompt(tool_tree_dict: dict, all_tools_description: list) -> str:
    """
    æ„å»ºä¸€ä¸ªæŒ‡å¯¼æ¨¡å‹è¿›è¡Œâ€œæ€è€ƒ->è¡ŒåŠ¨â€å¾ªç¯ï¼Œå¹¶æœ€ç»ˆåªè¾“å‡º \boxed{{}} ç­”æ¡ˆçš„ System Promptã€‚
    """
    tool_hierarchy_text = format_tool_tree_for_prompt(tool_tree_dict)
    tools_json_str = json.dumps(all_tools_description, indent=2, ensure_ascii=False)
    template = """You are a professional financial question answering assistant. Your task is to use tools to find the answer and provide ONLY the final result in a specific format.

# Workflow
1.  **Thinking & Tool Use**: Use the provided tools in a step-by-step manner to gather all necessary information. You can think and call tools for multiple turns.
2.  **Final Answer**: Once you have all the information, your **FINAL response MUST ONLY contain the `\\boxed{{}}` expression** and nothing else.

# Final Answer Requirements
- Your last response must start with `\\boxed{{` and end with `}}`.
- Inside `\\boxed{{}}`, place the final answer value(s) directly.
- **DO NOT** wrap the answer in a list `[]`.
- If the question asks for multiple values, separate them with a comma `,`.

---
### Interaction Example

#### User Question
What are the latest opening prices for SF Express's A-share (sz002352) and HK-share (0270)?

#### Your Response (First Turn)
Thinking:
I need to find the latest opening price for two different stocks in two different markets. I will use `get_a_stock_daily_price` for the A-share and `get_hk_stock_daily_price` for the HK-share.

Action:
<tool>get_a_stock_daily_price({{"code": "sz002352", "column_label": "open", "query_date": "latest"}})</tool>
<tool>get_hk_stock_daily_price({{"code": "0270", "column_label": "open", "query_date": "latest"}})</tool>

#### (System returns tool results to you: A-share price is 40.2, HK-share price is 7.2)

#### Your Response (Second and FINAL Turn) Put the answer in \\boxed{{}} (value, string, date) in the end of the answer, for example:
\\boxed{{40.2, 7.2}}
---

## Tool Hierarchy
{hierarchy}

## Available Tools (JSON Schema)
{tools_json}
"""
    system_prompt = template.format(hierarchy=tool_hierarchy_text, tools_json=tools_json_str)
    return system_prompt

def _numpy_converter(obj):
    if isinstance(obj, np.generic):
        return obj.item()
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    raise TypeError(f'Object of type {obj.__class__.__name__} is not JSON serializable')

# ==========================================================
# 5. æ ¸å¿ƒ Generate è¿‡ç¨‹ä¸è¯„ä¼°è¾…åŠ©å‡½æ•°
# ==========================================================
def execute_simplified_code(code_string: str, function_mapper_dict: dict) -> Tuple[bool, Any, Optional[str]]:
    """
    æ‰§è¡Œä»£ç å­—ç¬¦ä¸²
    """
    indented_code = "\n".join(["    " + line for line in code_string.strip().splitlines()])
    temp_func_code = f"def temp_solve():\n{indented_code}"
    try:
        execution_context = globals().copy()
        execution_context.update(function_mapper_dict)
        exec(temp_func_code, execution_context)
        solve_function = execution_context.get('temp_solve')
        if not callable(solve_function):
            return False, None, "Failed to create a callable temporary function from the code string."
        result = solve_function()
        return True, result, None
    except Exception as e:
        return False, None, f"Execution failed: {traceback.format_exc()}"


def _truncate_messages(messages: List[Dict[str, str]], tokenizer: Any, max_len: int) -> List[Dict[str, str]]:
    PROMPT_OVERHEAD_ESTIMATE = 50
    try:
        current_tokens = len(tokenizer.apply_chat_template(messages, add_generation_prompt=False))
    except Exception:
        current_tokens = sum(len(tokenizer.encode(m.get("content", ""))) for m in messages)
    if current_tokens + PROMPT_OVERHEAD_ESTIMATE <= max_len:
        return messages
    print(f"    [Agent è­¦å‘Š] å¯¹è¯å†å²è¿‡é•¿ (ä¼°ç®— {current_tokens} tokens)ï¼Œæ­£åœ¨è¿›è¡Œæˆªæ–­...")
    system_prompt = messages[0]
    CONVERSATION_TO_KEEP = 4 
    truncated_history = messages[-(CONVERSATION_TO_KEEP * 2):]
    new_messages = [system_prompt] + truncated_history
    try:
        new_tokens = len(tokenizer.apply_chat_template(new_messages, add_generation_prompt=False))
        print(f"    [Agent ä¿¡æ¯] æˆªæ–­åï¼Œå¯¹è¯å†å²é•¿åº¦ä» {current_tokens} å‡å°‘åˆ° {new_tokens} tokensã€‚")
    except Exception:
        pass
    return new_messages

def run_agent_loop(user_question: str, model: LLM, tokenizer: Any, sampling_params: SamplingParams, system_prompt: str) -> Tuple[str, str]:
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_question}]
    final_response = ""
    full_trajectory = f"--- USER ---\n{user_question}\n\n"
    try:
        model_max_len = model.llm_engine.model_config.max_model_len
    except AttributeError:
        model_max_len = 32768
        print(f"    [Agent è­¦å‘Š] æ— æ³•è‡ªåŠ¨è·å–æ¨¡å‹æœ€å¤§é•¿åº¦ï¼Œä½¿ç”¨é»˜è®¤å€¼: {model_max_len}")
    for turn in range(MAX_TURNS):
        print(f"    [Agent] æ­£åœ¨è¿›è¡Œç¬¬ {turn + 1}/{MAX_TURNS} æ¨ç†...")
        messages = _truncate_messages(messages, tokenizer, model_max_len)
        try:
            prompt_str = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        except Exception as e:
            error_msg = f"    [Agent é”™è¯¯] apply_chat_template å¤±è´¥: {e}"
            print(error_msg)
            return "", full_trajectory + error_msg
        if len(tokenizer.encode(prompt_str)) >= model_max_len:
            error_msg = f"    [Agent é”™è¯¯] å³ä½¿æˆªæ–­åï¼Œæœ€ç»ˆçš„ prompt é•¿åº¦ä»ç„¶è¶…å‡ºæ¨¡å‹é™åˆ¶ã€‚è¯·è€ƒè™‘å‡å°‘ CONVERSATION_TO_KEEP çš„å€¼ã€‚"
            print(error_msg)
            return "", full_trajectory + error_msg
        outputs = model.generate([prompt_str], sampling_params, use_tqdm=False)
        response_text = outputs[0].outputs[0].text.strip()
        messages.append({"role": "assistant", "content": response_text})
        final_response = response_text
        full_trajectory += f"--- ASSISTANT (æ€è€ƒä¸å·¥å…·è°ƒç”¨) ---\n{response_text}\n\n"
        print(f"    [Agent æ¨¡å‹å›å¤] {response_text[:150]}...")
        tool_call_matches = re.findall(r"<tool>(.*?)</tool>", response_text, re.DOTALL)
        if not tool_call_matches:
            print("    [Agent] æµç¨‹ç»“æŸ (æ¨¡å‹æœªè°ƒç”¨å·¥å…·ï¼Œå‡†å¤‡æä¾›æœ€ç»ˆç­”æ¡ˆ)ã€‚")
            return final_response, full_trajectory
        tool_output_content = ""
        for match_str in tool_call_matches:
            try:
                func_name = match_str.split('(', 1)[0].strip()
                args_str = match_str.rsplit(')', 1)[0].split('(', 1)[1]
                arguments = json.loads(args_str)
                function_to_call = function_mapper.get(func_name)
                if function_to_call:
                    function_output = function_to_call(**arguments)
                    output_str = json.dumps(function_output, ensure_ascii=False) if isinstance(function_output, (dict, list)) else str(function_output)
                    tool_output_content += output_str
                else:
                    tool_output_content += f"Error: Tool '{func_name}' not found."
            except Exception as e:
                tool_output_content += f"å·¥å…·æ‰§è¡Œå¤±è´¥: {e}"
        if len(tool_output_content) > 4000:
            print(f"    [Agent è­¦å‘Š] å·¥å…·è¾“å‡ºè¿‡é•¿ ({len(tool_output_content)} chars)ï¼Œå°†è¢«æˆªæ–­ã€‚")
            tool_output_content = tool_output_content[:4000] + "\n... [TRUNCATED] ..."
        messages.append({"role": "tool", "content": tool_output_content})
        full_trajectory += f"--- TOOL OUTPUT ---\n{tool_output_content}\n\n"
    print(f"    [Agent è­¦å‘Š] è¾¾åˆ°æœ€å¤§ {MAX_TURNS} è½®æ¬¡ï¼Œå¼ºè¡Œç»ˆæ­¢ã€‚")
    return final_response, full_trajectory

def extract_boxed_answer(model_content: str) -> Tuple[Any, Optional[str]]:
    boxed_match = re.search(r"\\boxed{([^}]+)}", model_content)
    if not boxed_match:
        return None, "Error: No \\boxed{} found in the final response."
    boxed_answer_str = boxed_match.group(1).strip()
    eval_str = f"({boxed_answer_str},)" if ',' not in boxed_answer_str else f"({boxed_answer_str})"
    try:
        import ast
        result = ast.literal_eval(eval_str)
        return result[0] if len(result) == 1 else result, None
    except Exception as e:
        return None, f"Error: Failed to parse content inside \\boxed{{}}: {e}"

def compare_answers(model_answer: Any, ground_truth: Any, rel_tol=1e-5) -> bool:
    # 1. æ ‡å‡†åŒ–ä¸ºå…ƒç»„ä»¥ä¾¿ç»Ÿä¸€æ¯”è¾ƒ
    model_tuple = model_answer if isinstance(model_answer, tuple) else (model_answer,)
    truth_tuple = ground_truth if isinstance(ground_truth, tuple) else (ground_truth,)
    # 2. æ£€æŸ¥é•¿åº¦
    if len(model_tuple) != len(truth_tuple):
        return False
    # 3. é€ä¸ªå…ƒç´ æ¯”è¾ƒ
    for m_item, t_item in zip(model_tuple, truth_tuple):
        if isinstance(m_item, (int, float)) and isinstance(t_item, (int, float)):
            if not math.isclose(m_item, t_item, rel_tol=rel_tol, abs_tol=1e-9):
                return False
        elif isinstance(m_item, float) and isinstance(t_item, float) and math.isnan(m_item) and math.isnan(t_item):
            continue
        elif m_item != t_item:
            return False
    return True

# ==========================================================
# 6. ä¸»è¯„ä¼°å¾ªç¯ (å·²ä¿®å¤ vLLM åŠ è½½é—®é¢˜)
# ==========================================================
def main_evaluation(model: LLM, tokenizer: Any, sampling_params: SamplingParams):
    
    system_prompt = build_final_answer_prompt(tool_tree, all_tools_description)
    print("--- å¯åŠ¨è¯„ä¼° (Agentæµç¨‹ + ç›´æ¥å€¼Boxed Answerè¯„ä¼°) ---")
    
    try:
        with open(DATASET_FILE_PATH, 'r', encoding='utf-8') as f: data_list = json.load(f)
        total_count = len(data_list)
        print(f"æˆåŠŸåŠ è½½ {total_count} æ¡æ•°æ®ã€‚")
    except Exception as e: 
        print(f"ã€è‡´å‘½é”™è¯¯ã€‘: åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        return
        
    processed_prompts = set()
    if os.path.exists(OUTPUT_FILE_PATH):
        try:
            with open(OUTPUT_FILE_PATH, 'r', encoding='utf-8') as f_in:
                for line in f_in:
                    if line.strip():
                        try:
                            data = json.loads(line)
                            if "prompt" in data:
                                processed_prompts.add(data["prompt"])
                        except json.JSONDecodeError:
                            print(f"  [è­¦å‘Š] åœ¨è¯»å–è¾“å‡ºæ–‡ä»¶æ—¶å‘ç°æ— æ•ˆJSONè¡Œï¼Œå·²è·³è¿‡: {line.strip()}")
                            pass
            print(f"--- å·²åŠ è½½ {len(processed_prompts)} ä¸ªå·²å¤„ç†çš„ promptsï¼Œå°†åœ¨æœ¬æ¬¡è¿è¡Œä¸­è·³è¿‡å®ƒä»¬ã€‚---\n")
        except Exception as e:
            print(f"--- è¯»å–ç°æœ‰è¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}. å°†ä»å¤´å¼€å§‹å¤„ç†ã€‚ ---")
            
    results_log = [] 
    success_count_this_run = 0
    
    with open(OUTPUT_FILE_PATH, 'a', encoding='utf-8') as f_out:
        for i, item in enumerate(data_list):
            prompt = item.get("question")
            ground_truth_code = item.get("code")
            
            if prompt in processed_prompts:
                continue
                
            print(f"\n--- æ­£åœ¨è¯„ä¼°ç¬¬ {i+1}/{total_count} é¡¹ (æ–°é¡¹ç›®) ---")
            
            if not prompt or not ground_truth_code: 
                print("    [è·³è¿‡] æ•°æ®ç¼ºå¤± ('question' or 'code')ã€‚")
                continue
                
            print(f"    [Prompt] {prompt[:100]}...")
            
            log_entry = { 
                        "index": i + 1, 
                        "prompt": prompt, 
                        "is_correct": False, 
                        "eval_score": 0 
                        }
            
            success, ground_truth_answer, err = execute_simplified_code(ground_truth_code, function_mapper)
            
            if not success:
                print(f"    âŒ åŸºå‡†ä»£ç æ‰§è¡Œå¤±è´¥: {err}")
                log_entry["ground_truth_error"] = str(err)
                f_out.write(json.dumps(log_entry, ensure_ascii=False, default=_numpy_converter) + '\n'); f_out.flush()
                processed_prompts.add(prompt)
                continue
                
            log_entry["ground_truth_answer"] = ground_truth_answer
            print(f"    âœ… åŸºå‡†ç­”æ¡ˆ: {ground_truth_answer}")
            
            # [ä¿®å¤] (tom)ï¼Œ'model', 'tokenizer', 'sampling_params' ç°åœ¨æ˜¯ä¼ å…¥çš„å‚æ•°
            final_response, trajectory = run_agent_loop(prompt, model, tokenizer, sampling_params, system_prompt)
            log_entry["model_trajectory"] = trajectory
            
            model_answer, err = extract_boxed_answer(final_response)
            
            if err:
                print(f"    âŒ æ¨¡å‹ç­”æ¡ˆæå–å¤±è´¥: {err}")
                log_entry["model_error"] = err
                f_out.write(json.dumps(log_entry, ensure_ascii=False, default=_numpy_converter) + '\n'); f_out.flush()
                processed_prompts.add(prompt)
                continue
                
            log_entry["model_answer"] = model_answer
            print(f"    ğŸ¤– æ¨¡å‹ç­”æ¡ˆ: {model_answer}")
            
            is_correct = compare_answers(model_answer, ground_truth_answer)
            log_entry["is_correct"] = is_correct
            
            if is_correct:
                log_entry["eval_score"] = 1
                success_count_this_run += 1
                print("    ğŸ‘ ç»“æœ: æ­£ç¡®")
            else:
                log_entry["eval_score"] = 0
                print("    ğŸ‘ ç»“æœ: é”™è¯¯")
                
            json_str = json.dumps(log_entry, ensure_ascii=False, default=_numpy_converter)
            f_out.write(json_str + '\n')
            f_out.flush()
            processed_prompts.add(prompt)
            results_log.append(log_entry) 
            
    total_processed_in_file = len(processed_prompts)
    processed_this_run = len(results_log)
    total_correct_in_file = 0
    
    if os.path.exists(OUTPUT_FILE_PATH):
        with open(OUTPUT_FILE_PATH, 'r', encoding='utf-8') as f_final:
            for line in f_final:
                if line.strip():
                    try:
                        final_data = json.loads(line)
                        if final_data.get("is_correct") is True:
                            total_correct_in_file += 1
                    except: pass
                    
    print("\n" + "="*50)
    print("--- è¯„ä¼°æ€»ç»“ ---")
    print(f"æœ¬æ¬¡è¿è¡Œå¤„ç†æ¡ç›®:  {processed_this_run}")
    print(f"  - æœ¬æ¬¡è¿è¡Œæ­£ç¡®:  {success_count_this_run}")
    print("-" * 20)
    print(f"è¾“å…¥æ–‡ä»¶æ€»æ¡ç›®:      {total_count}")
    print(f"è¾“å‡ºæ–‡ä»¶ä¸­æ€»å·²å¤„ç†:  {total_processed_in_file}")
    print(f"è¾“å‡ºæ–‡ä»¶ä¸­æ€»æ­£ç¡®:    {total_correct_in_file}")
    
    if total_processed_in_file > 0:
        overall_accuracy = (total_correct_in_file / total_processed_in_file) * 100
        print(f"åŸºäºå·²å¤„ç†æ¡ç›®çš„æ€»æˆåŠŸç‡: {overall_accuracy:.2f}%")
        
    print(f"è¯¦ç»†æ—¥å¿—å·²è¿½åŠ åˆ°: {OUTPUT_FILE_PATH}")
    print("="*50)

# ==========================================================
# 7. ä¸»å…¥å£ç‚¹
# ==========================================================
if __name__ == "__main__":
    try:
        print(f"--- æ­£åœ¨åŠ è½½ VLLM æ¨¡å‹: {MODEL_PATH} ---")
        model = LLM(
            model=MODEL_PATH, 
            tensor_parallel_size=TENSOR_PARALLEL_SIZE, 
            gpu_memory_utilization=GPU_MEM_UTILIZATION,
            max_model_len=MAX_MODEL_LEN,
            trust_remote_code=True,
            disable_log_stats=True 
        )
        tokenizer = model.get_tokenizer()
        sampling_params = SamplingParams(temperature=0.0, max_tokens=MAX_TOKENS) 
        print("--- VLLM æ¨¡å‹å’Œ Tokenizer åŠ è½½æˆåŠŸã€‚")
    except Exception as e:
        print(f"åŠ è½½ vLLM æ¨¡å‹å¤±è´¥: {e}"); 
        print(traceback.format_exc()) 
        sys.exit(1)
    main_evaluation(model, tokenizer, sampling_params)